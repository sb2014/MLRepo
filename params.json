{"name":"Mlrepo","tagline":"","body":"---\r\ntitle: \"Machine Learning - Course Project\"\r\nauthor: \"SB\"\r\ndate: \"August 16, 2014\"\r\noutput: html_document\r\n---\r\n\r\nSix young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:  \r\nClass A: Exactly according to the specification  \r\nClass B: Throwing the elbows to the front  \r\nClass C: Lifting the dumbbell only halfway  \r\nClass D: Lowering the dumbbell only halfway   \r\nClass E: Throwing the hips to the front  \r\n  \r\nTraining Data has been provided to reflect the resulting 'Class' of each observation from the above exercise performed by the six participants. The goal is to determine the best 'Prediction Model', which can be used to predict the 'Class' for the Test data provided separately  \r\n\r\n\r\nSynopsis:  \r\n---------------------------------------------------------------------------------------------\r\n* Since the goal of the exercise is to classify the Test data into one of the five 'Classes' defined in the Training data, different classification models have been used to test the accuracy of the models\r\n* The Training data will be further divided into 'Training' and 'Testing' sets in the ratio of 70/30. Each classification model will be run against the 'Training' set and cross-validated against the 'Testing' set. The model with highest accuracy will be selected to predict the 'classe' for the Testing Data provided.\r\n\r\nData Processing:  \r\n--------------------------------------------------------------------------------------------\r\n* Download the files to the working directory    \r\n\r\n```{r}\r\nif(!file.exists(\"training.csv\")) {\r\n    fileTrain<- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\n    download.file(fileTrain, destfile= \"./training.csv\", method=\"curl\")\r\n}\r\n\r\nif(!file.exists(\"testing.csv\")) {\r\n    fileTest<- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n    download.file(fileTest, destfile= \"./testing.csv\", method=\"curl\")\r\n}\r\n```\r\n\r\n* Load the data into Training and Testing data frames  \r\n\r\n```{r}\r\ntrainingData<- read.table(\"training.csv\", header=TRUE, sep=\",\")\r\ntestingData<- read.table(\"testing.csv\", header=TRUE, sep=\",\")\r\n```\r\n\r\n* Feature Selection: Since the model will finally be run against the Testing Data, summarize the Testing Data and create a sub-set Training Data set and Testing Data set with only numerical columns not missing majority of the values (NAs)  \r\n\r\n```{r}\r\ntrainSet <- trainingData[,c(\"roll_belt\",\"pitch_belt\",\"yaw_belt\", \"total_accel_belt\", \"gyros_belt_x\",\"gyros_belt_y\",\"gyros_belt_z\",\"accel_belt_x\",\"accel_belt_y\",\"accel_belt_z\",\"magnet_belt_x\",\"magnet_belt_y\",\"magnet_belt_z\",\"roll_arm\",\"pitch_arm\",\"yaw_arm\",\"total_accel_arm\",\"gyros_arm_x\",\"gyros_arm_y\",\"gyros_arm_z\",\"accel_arm_x\",\"accel_arm_y\",\"accel_arm_z\",\"magnet_arm_x\",\"magnet_arm_y\",\"magnet_arm_z\",\"roll_dumbbell\",\"pitch_dumbbell\",\"yaw_dumbbell\",\"total_accel_dumbbell\",\"gyros_dumbbell_x\",\"gyros_dumbbell_y\",\"gyros_dumbbell_z\",\"accel_dumbbell_x\",\"accel_dumbbell_y\",\"accel_dumbbell_z\",\"magnet_dumbbell_x\",\"magnet_dumbbell_y\",\"magnet_dumbbell_z\",\"roll_forearm\",\"pitch_forearm\",\"yaw_forearm\",\"total_accel_forearm\",\"gyros_forearm_x\",\"gyros_forearm_y\",\"gyros_forearm_z\",\"accel_forearm_x\",\"accel_forearm_y\",\"accel_forearm_z\",\"magnet_forearm_x\",\"magnet_forearm_y\",\"magnet_forearm_z\", \"classe\")]\r\ntestSet <- testingData[,c(\"roll_belt\",\"pitch_belt\",\"yaw_belt\", \"total_accel_belt\", \"gyros_belt_x\",\"gyros_belt_y\",\"gyros_belt_z\",\"accel_belt_x\",\"accel_belt_y\",\"accel_belt_z\",\"magnet_belt_x\",\"magnet_belt_y\",\"magnet_belt_z\",\"roll_arm\",\"pitch_arm\",\"yaw_arm\",\"total_accel_arm\",\"gyros_arm_x\",\"gyros_arm_y\",\"gyros_arm_z\",\"accel_arm_x\",\"accel_arm_y\",\"accel_arm_z\",\"magnet_arm_x\",\"magnet_arm_y\",\"magnet_arm_z\",\"roll_dumbbell\",\"pitch_dumbbell\",\"yaw_dumbbell\",\"total_accel_dumbbell\",\"gyros_dumbbell_x\",\"gyros_dumbbell_y\",\"gyros_dumbbell_z\",\"accel_dumbbell_x\",\"accel_dumbbell_y\",\"accel_dumbbell_z\",\"magnet_dumbbell_x\",\"magnet_dumbbell_y\",\"magnet_dumbbell_z\",\"roll_forearm\",\"pitch_forearm\",\"yaw_forearm\",\"total_accel_forearm\",\"gyros_forearm_x\",\"gyros_forearm_y\",\"gyros_forearm_z\",\"accel_forearm_x\",\"accel_forearm_y\",\"accel_forearm_z\",\"magnet_forearm_x\",\"magnet_forearm_y\",\"magnet_forearm_z\")]\r\n\r\n```\r\n\r\n* Load the required libraries. Divide the trainingData into Training and Test sets  in 70/30 ratio.\r\n\r\n```{r message=FALSE, warning=FALSE}\r\nlibrary(caret); library(rpart); library(randomForest); library(psych); library(rattle)\r\ninTrain <- createDataPartition (y= trainSet$classe, p=0.7, list = FALSE)\r\ntrainSub <- trainSet[inTrain,]\r\ntestSub <- trainSet[-inTrain,]\r\n```\r\n\r\nRecursive Partitioning Method:\r\n-----------------------------------------------------------------------------------------\r\n* Train the dataset using the 'Recursive Partitioning' method. Show the resulting classification tree\r\n\r\n```{r message=FALSE, warning=FALSE}\r\nmodelRpart <- train(classe~., method=\"rpart\", data=trainSub)\r\nfancyRpartPlot(modelRpart$finalModel)\r\n```\r\n\r\n_Expected 'Out of Sample Error' of this model will be (1-Accuracy) of the row with least Complexity Parameter (cp) value shown below_    \r\n```{r}\r\nmodelRpart\r\n```\r\n  \r\n* Predict the values for 'Classe' variable by applying the trained model to the test sub-set.  \r\n```{r}\r\npredRpart <- predict(modelRpart,testSub)\r\n```\r\n  \r\n_'Estimated Error' of this model based on the cross-validation with the Test subset is represented by (1- Overall Accuracy) shown in the resuls of the Confusion Matrix below:_  \r\n```{r}\r\nconfusionMatrix(predRpart, testSub$classe)\r\n```\r\n\r\nRandom Forest Method:\r\n-----------------------------------------------------------------------------------------\r\n\r\n* Train the dataset using the 'Random Forest' method. To avoid long computational times, we are limiting the number of cross-validations to 5 through TrainControl parameter  \r\n\r\n```{r message=FALSE, warning=FALSE}\r\nfitControl = trainControl(method = \"cv\", number = 5)\r\nmodelRf <- train(classe~., method=\"rf\", trControl=fitControl, data=trainSub, prox=TRUE) \r\n```\r\n\r\n_Expected 'Out of Sample Error' of this model will be (1-Accuracy) of the row with least Complexity Parameter (cp) value shown below_    \r\n```{r}\r\nmodelRf\r\n```\r\n  \r\n* Predict the values for 'Classe' variable by applying the trained model to the test sub-set.  \r\n```{r}\r\npredRf <- predict(modelRf,testSub)\r\n```\r\n  \r\n_'Estimated Error' of this model based on the cross-validation with the Test subset is represented by (1- Overall Accuracy) shown in the resuls of the Confusion Matrix below:_  \r\n```{r}\r\nconfusionMatrix(predRf, testSub$classe)\r\n```\r\n\r\n\r\nBoosting Method:\r\n-----------------------------------------------------------------------------------------\r\n* Train the dataset using the 'Boosting' method. To avoid long computational times, we are limiting the number of cross-validations to 5 through Train Control parameter  \r\n\r\n```{r message=FALSE, warning=FALSE}\r\nfitControl = trainControl(method = \"cv\", number = 5)\r\nmodelGbm <- train(classe ~., method=\"gbm\", trControl=fitControl, data=trainSub, verbose=FALSE)\r\n```\r\n\r\n_Expected 'Out of Sample Error' of this model will be (1-Accuracy) of the row with least Complexity Parameter (cp) value shown below_    \r\n```{r}\r\nmodelGbm\r\n```\r\n  \r\n* Predict the values for 'Classe' variable by applying the trained model to the test sub-set.\r\n```{r}\r\npredGbm <- predict(modelGbm,testSub)\r\n```\r\n  \r\n_'Estimated Error' of this model based on the cross-validation with the Test subset is represented by (1- Overall Accuracy) shown in the resuls of the Confusion Matrix below:_  \r\n```{r}\r\nconfusionMatrix(predGbm, testSub$classe)\r\n```\r\n\r\n\r\nConclusions:\r\n----------------------------------------------------------------------------------------------\r\n* As shown in the results of the confusionMatrix for each of the models, Random Forest method offers the highest accuracy, followed by Boosting method. Lowest accuracy is offered by the simple 'Recursive Partitioning' method out of the three models.\r\n\r\n* Based on these results, we choose to use the Random Forest model to predict the results for the Testing Data\r\n\r\n```{r}\r\npredVal <- predict(modelRf, testSet)\r\npredVal\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}